# -*- coding: utf-8 -*-
"""대학원 스마트 도서관.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLG2v0MvZfJQODZN7fUcE_zOzADyXcQ-

# 기본 세팅
"""

# Commented out IPython magic to ensure Python compatibility.
#한글 깨짐 방지

# %config InlineBackend.figure_format = "retina"
import warnings
warnings.filterwarnings(action='ignore')
from IPython.display import display
from IPython.display import HTML

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic')

# 데이터 처리
import pandas as pd
import numpy as np
from collections import OrderedDict

# 지도/공간정보 처리
import geopandas as gpd
from shapely import wkt, geometry
from shapely.geometry import Point
from pyproj import Proj, transform

# 시각화
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap
from matplotlib.ticker import MaxNLocator
import seaborn as sns
import folium
import plotly.express as px
import plotly.graph_objs as go

# 기계학습 및 통계
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.neighbors import KDTree
from statsmodels.stats.outliers_influence import variance_inflation_factor

# 기타
import os
import sys

"""# df1 생성 및 전처리(상관계수 확인, VIF 계수 확인, 변수 제거, 파생변수 생성)"""

df = pd.read_csv('/content/drive/MyDrive/[DATA] 도서관 데이터 공모전/군집화용데이터_V2.csv',encoding='utf-8') #울산 행정동별 정보 데이터

df

#변수별 히트맵
df_corr = df.iloc[:, 2:].replace(',', '', regex=True).astype(float)
corr_matrix = df_corr.corr()
plt.figure(figsize=(20, 18))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, annot_kws={"size": 7})
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Heatmap')
plt.show()

#연령별 인구수, 교통 추정량 삭제
age_columns = df.filter(regex=r'^\d{1,2}~\d{1,2}세$').columns
df1 = df.drop(columns=age_columns)
df1=df1.drop(columns=['장애인 수','미성년인구수','청년인구수','노인인구수','성인인구수'])
df1=df1.drop(columns=['론볼경기장수','노인비율','사격장수'])
df1=df1.drop(columns=['구기체육관수'])
df1=df1.drop(columns=['승용차의추정교통량','버스의추정교통량','화물차의추정교통량'])

#의미없는 여가시설 개수 삭제
df1=df1.drop(columns=['풋살장수', '생활체육관수', '다목적구장수', '국궁장수', '테니스장수', '게이트볼장수', '축구장수', '수영장수',
       '족구장수', '스쿼시장수', '롤러스케이트장수', '야구장수', '씨름장수', '배드민턴장수',
       '양궁장수', '클라이밍장수', '농구장수', '수상레저시설수', '대규모점포수'])

df1=df1.drop(columns=['미성년비율','청년비율']) #VIF
df1=df1.drop(columns=['헬스장수','골프장수']) #상관관계
df1=df1.drop(columns=['버스정류장수']) #VIF계수
df1=df1.drop(columns=['PC방수','총인구수']) #상관관계
df1=df1.drop(columns=['CCTV수','산수']) #상관계수
df1=df1.drop(columns=['자전거보관소수']) #상관계수
df1=df1.drop(columns=['100세 이상']) #의미가 없다 판단
df1=df1.drop(columns=['공원수','공영주차장수']) #의미가 없다 판단

df_corr1 = df1.iloc[:, 2:].replace(',', '', regex=True).astype(float)
corr_matrix1 = df_corr1.corr()
plt.figure(figsize=(20, 18))
sns.heatmap(corr_matrix1, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, annot_kws={"size": 7})
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Heatmap')
plt.show()

#변수별 VIF 계수 확인
X = df1.iloc[:, 2:]
X = X.replace(',', '', regex=True).astype(float)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
vif_data

"""# df3 생성 및 전처리(상관계수 확인, 변수 제거, 파생변수 생성)"""

df3 = df1.copy()

#파생변수 생성
df['사업체 종사자 수'] = df['사업체 종사자 수'].replace(',', '', regex=True).astype(float)
df['총인구수'] = df['총인구수'].replace(',', '', regex=True).astype(float)
df['사업체종사자비율'] = df['사업체 종사자 수'] / df['총인구수']
df['기초생활수급자 수'] = df['기초생활수급자 수'].apply(pd.to_numeric, errors='coerce')
df['총인구수'] = df['총인구수'].apply(pd.to_numeric, errors='coerce')
df['기초생활수급자비율'] = df['기초생활수급자 수'] / df['총인구수']
df3['사업체종사자비율'] = df['사업체종사자비율']
df3['기초생활수급자비율'] = df['기초생활수급자비율']
df3['면적당여가시설수'] = df3['총여가시설수']/df3['면적']

#변수 삭제
df3=df3.drop(columns=['사업체 종사자 수','성인비율'])

#df3 상관관계 히트맵
df_corr3 = df3.iloc[:, 2:].replace(',', '', regex=True).astype(float)
corr_matrix3 = df_corr3.corr()
plt.figure(figsize=(20, 18))
sns.heatmap(corr_matrix3, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, annot_kws={"size": 7})
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Heatmap')
plt.show()

"""# df4 생성 및 전처리(상관계수 확인, VIF계수 확인, 파생변수 생성)"""

df

df3

#df3에서 괜찮았던 칼럼들 추가 및 파생변수 생성
columns_to_keep = ['행정동','시군구','유동인구수','기초생활수급자비율']
df4 = df3[columns_to_keep]
df4['면적당건물수'] = df3['건물수']/df3['면적']
df4['면적당장애인복지시설수'] = df3['장애인복지시설수']/df3['면적']
df4['면적당노인복지시설수'] = df3['노인복지시설수']/df3['면적']
df4['면적당도서관수'] = df3['도서관수']/df3['면적']
df4['면적당여가시설수'] = df3['총여가시설수']/df3['면적']

df4

#상관관계 히트맵
df_corr4 = df4.iloc[:, 2:].replace(',', '', regex=True).astype(float)
corr_matrix4 = df_corr4.corr()
plt.figure(figsize=(20, 18))
sns.heatmap(corr_matrix4, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, annot_kws={"size": 7})
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title('Correlation Heatmap')
plt.show()

# VIF 계산
X = df4.iloc[:, 2:]
X = X.replace(',', '', regex=True).astype(float)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
vif_data

"""# df4에 대해서 차원축소 및 군집화 진행(PCA + K-Means/GMM 성능 비교) -> 주성분 3개, 클러스터 4개일 때가 최적이라고 판단!"""

#PCA 개수에 따른 GMM/K-Means 성능 분석

X = df4.iloc[:, 2:].replace(',', '', regex=True).astype(float)
X_scaled = StandardScaler().fit_transform(X)

pca_range = range(2, 5)
cluster_range = range(2, 11)

results = {
    'pca_components': [],
    'n_clusters': [],
    'model': [],
    'silhouette_score': []
}

for n_components in pca_range:
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)

    for n_clusters in cluster_range:
        # K-means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        km_labels = kmeans.fit_predict(X_pca)
        results['pca_components'].append(n_components)
        results['n_clusters'].append(n_clusters)
        results['model'].append('K-means')
        results['silhouette_score'].append(silhouette_score(X_pca, km_labels))

        # GMM
        gmm = GaussianMixture(n_components=n_clusters, random_state=42)
        gm_labels = gmm.fit_predict(X_pca)
        results['pca_components'].append(n_components)
        results['n_clusters'].append(n_clusters)
        results['model'].append('GMM')
        results['silhouette_score'].append(silhouette_score(X_pca, gm_labels))

results_df = pd.DataFrame(results)


# 누적 분산 설명 비율 시각화
plt.figure(figsize=(10, 6))
for n_components in pca_range:
    pca = PCA(n_components=n_components)
    pca.fit(X_scaled)
    cum_var = np.cumsum(pca.explained_variance_ratio_)
    plt.plot(
        range(1, n_components + 1),
        cum_var,
        marker='o',
        label=f'PCA={n_components}' if n_components == max(pca_range) else None
    )

plt.axhline(y=0.7, linestyle='--', color='r', label='70% Threshold')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Cumulative Explained Variance Ratio by PCA Size')
plt.legend()
plt.grid(True)
plt.show()


# PCA별 군집화 기법 별 성능 시각화
plt.figure(figsize=(14, 7))

for n_components in pca_range:
    # K-means
    subset_k = results_df[
        (results_df['model'] == 'K-means') &
        (results_df['pca_components'] == n_components)
    ]
    plt.plot(
        subset_k['n_clusters'],
        subset_k['silhouette_score'],
        marker='o',
        linestyle='-',
        label=f'K-means (PCA={n_components})'
    )

    # GMM
    subset_g = results_df[
        (results_df['model'] == 'GMM') &
        (results_df['pca_components'] == n_components)
    ]
    plt.plot(
        subset_g['n_clusters'],
        subset_g['silhouette_score'],
        marker='s',
        linestyle='--',
        label=f'GMM (PCA={n_components})'
    )

plt.xlabel('Number of Clusters')
plt.ylabel('Average Silhouette Score')
plt.title('Silhouette Score vs. Number of Clusters')
plt.legend()
plt.grid(True)
plt.show()

# 주성분 로딩값 시각화
def pca_and_plot(n_components, X_scaled):
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)

    components = pca.components_
    loadings_df = pd.DataFrame(components.T, columns=[f'PC{i+1}' for i in range(n_components)], index=X.columns)

    plt.figure(figsize=(12, 8))
    sns.heatmap(loadings_df, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
    plt.title(f'PCA Loadings for {n_components} Components')
    plt.show()

    return loadings_df

#PCA2
loadings_2 = pca_and_plot(2, X_scaled)
print("PCA with 2 components loadings:")
print(loadings_2)

#PCA3
loadings_3 = pca_and_plot(3, X_scaled)
print("PCA with 3 components loadings:")
print(loadings_3)

#PCA4
loadings_4 = pca_and_plot(4, X_scaled)
print("PCA with 4 components loadings:")
print(loadings_4)

#누적 분산 설명 비율
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

print(f"각 주성분의 분산 설명 비율: {explained_variance_ratio}")
print(f"누적 분산 설명 비율: {cumulative_explained_variance}")

# PCA -> 주성분 3개, 군집화 기법 -> K-Means, 군집 개수 -> 4개 일때가 최적이라 판단
X = df4.iloc[:, 2:]
X = X.replace(',', '', regex=True).astype(float)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans_labels = kmeans.fit_predict(X_pca)

silhouette_avg = silhouette_score(X_pca, kmeans_labels)
print(f'K-means with {n_clusters} Clusters\nSilhouette Score: {silhouette_avg:.4f}')

# PCA 결과와 군집 레이블을 df4에 추가
df4['PC1'] = X_pca[:, 0]
df4['PC2'] = X_pca[:, 1]
df4['PC3'] = X_pca[:, 2]
df4['Cluster'] = kmeans_labels

df4

#시군구별 군집 개수 확인
df4.groupby(['시군구', 'Cluster']).size().unstack(fill_value=0)

#군집별 PCA 결과 시각화
fig = px.scatter_3d(df4, x='PC1', y='PC2', z='PC3', color='Cluster',
                    title="3D PCA Visualization by Cluster")

fig.update_layout(
    scene=dict(
        xaxis=dict(range=[df4['PC1'].min() - 2, df4['PC1'].max() + 2]),
        yaxis=dict(range=[df4['PC2'].min() - 2, df4['PC2'].max() + 2]),
        zaxis=dict(range=[df4['PC3'].min() - 2, df4['PC3'].max() + 2])
    ),
    width=1200,
    height=800
)

fig.show()

df4.to_csv("군집화완료(Kmeans(4)_PCA(3)).csv",encoding='utf-8-sig')

"""# df_cluster 생성(울산시 시군구별 클러스터 Mapping)"""

df

# 행정동의 모든 정보를 갖고 있는 df_cluster 생성
df_cluster = pd.concat([df, df4.iloc[:, -4:]], axis=1)

df_cluster.head()

"""# MCLP 대상 군집선정 -> PCA 성분 분석 -> 0번, 3번 군집에 대해서 MCLP 입지선정 하기로 결정"""

df_cluster.head()

# PCA 성분별 박스 플롯 시각화
plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
sns.boxplot(x='Cluster', y='PC1', data=df_cluster)
plt.title('PC1 by Cluster')

plt.subplot(1, 3, 2)
sns.boxplot(x='Cluster', y='PC2', data=df_cluster)
plt.title('PC2 by Cluster')
plt.subplot(1, 3, 3)
sns.boxplot(x='Cluster', y='PC3', data=df_cluster)
plt.title('PC3 by Cluster')

plt.tight_layout()
plt.show()

df_cluster.columns

# 클러스터별 주요 지표 평균값 계산
columns_to_plot = ['장애인 수', '기초생활수급자 수','총여가시설수', '모든차량의추정교통량','면적',
                   '미성년비율', '청년비율', '노인비율', '성인비율','유동인구수',
                   '장애인복지시설수', '노인복지시설수','CCTV수']
df_cluster_mean = df_cluster.groupby('Cluster')[columns_to_plot].mean().reset_index()
num_cols = 3
num_rows = (len(columns_to_plot) + num_cols - 1) // num_cols
fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, num_rows * 5))
axes = axes.flatten()
for i, col in enumerate(columns_to_plot):
    sns.barplot(data=df_cluster_mean, x='Cluster', y=col, ax=axes[i], palette='Blues_d')
    axes[i].set_title(f'Cluster별 {col} 평균값', fontsize=14)
    axes[i].set_xlabel('Cluster', fontsize=12)
    axes[i].set_ylabel(col, fontsize=12)
    axes[i].tick_params(axis='x', rotation=45)
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""군집별 특징

*   0번군집 - PC2 최대 (남구4 중구4 동구2)
*   1번군집 - PC1 최소
*   2번군집 - PC1 최대, PC3최소
*   3번군집 - PC2 최소, PC3최대 (남구3)

*   PC1(45%): 건물수(0.5), 면적당 여가시설수(0.48), 면적당 노인복지시설수(0.47), 면적당 도서관수(0.47)
*   PC2(20%): 기초생활수급자비율(0.61), 유동인구수(-0.55), 면적당장애인복지시설수(0.45)
*   PC3(15%): 유동인구수(0.75), 기초생활수급자비율(0.43), 면적당장애인복지시설수(0.28)

변수

*   사회적 안정성(CCTV) 0.085 - 20.57%
*   대중교통수단의 편리성(버스정류장) 0.094 - 22.76%
*   다른 시설과의 연계성(도서관, 주차장, 공원) 0.071 - 17.19%
*   봉사대상인구수(공동주택, 단독주택, 복지시설, 숙박시설, 유동인구수) 0.074 - 17.92%
*   여가시설과의 연계성(여가시설) 0.089 - 21.55%

# 후보지별 입지선정계수 생성(GIS건물통합정보 데이터 이용) -> df_mclp 생성
"""

#MCLP용 GIS 데이터프레임
df_gis = pd.read_csv('/content/drive/MyDrive/[DATA] 도서관 데이터 공모전/mclp용데이터V1.csv',encoding='utf-8')

df_gis.columns

df_gis = df_gis.drop(columns = ['안정성(a)', '편리성(b)',
       '연계성(c)', '봉사대상인구수(d)', 'weights'])

# MCLP 에서 사용할 후보지
df_hu = df_gis[df_gis['건축물용도명'].isin(['오락시설', '산림시설', '공공체육시설', '문화및집회시설', '운동시설'])]

#중복 확인
df_hu[df_hu.duplicated(subset=['위도', '경도','건물명'], keep=False)]

# 후보지 데이터에서 겹치는 후보지 제거
df_hu = df_hu.drop(index=[120392, 120561])

# 원본 데이터에서 겹치는 후보지 제거
df_gis = df_gis.drop(index=[120392, 120561])

df_hu

# 입지선정계수에 사용할 변수들 스케일링
scaler = MinMaxScaler()

columns_to_scale = [
    '반경300m내CCTV수',
    '반경2km내버스정류장수',
    '반경2km내도서관수',
    '반경2km내여가시설수',
    '반경1km내공영주차장수',
    '반경2.5km내공원수',
    '반경2km내단독주택수',
    '반경2km내공동주택수',
    '반경2km내숙박시설수',
    '반경2km내지식정보취약계층복지시설수',
    '유동인구'
]

df_hu[columns_to_scale] = scaler.fit_transform(df_hu[columns_to_scale])

# 입지선정계수 구성요소 별 가중치 설정
weights = {
    '사회적 안정성': 0.2057,
    '대중교통수단의 편리성': 0.2276,
    '다른시설과의 연계성': 0.1720,
    '봉사대상인구수': 0.1792,
    '여가시설과의 연계성' : 0.2155
}

# 입지선정계수 계산
df_hu['사회적 안정성(a)'] = df_hu['반경300m내CCTV수'] * weights['사회적 안정성']

df_hu['대중교통수단의 편리성(b)'] = df_hu['반경2km내버스정류장수'] * weights['대중교통수단의 편리성']

df_hu['다른시설과의 연계성(c)'] = (df_hu['반경2km내도서관수'] +
                            df_hu['반경1km내공영주차장수'] +
                            df_hu['반경2.5km내공원수']) * weights['다른시설과의 연계성']

df_hu['봉사대상인구수(d)'] = ((0.019)*df_hu['반경2km내단독주택수'] +
                         (0.381)*df_hu['반경2km내공동주택수'] +
                         (0.2)*df_hu['반경2km내숙박시설수'] +
                         (0.2)*df_hu['반경2km내지식정보취약계층복지시설수'] +
                         (0.2)*df_hu['유동인구']) * weights['봉사대상인구수']

df_hu['여가시설과의 연계성(e)'] = weights['여가시설과의 연계성'] * df_hu['반경2km내여가시설수']
df_hu['입지선정계수'] = df_hu['사회적 안정성(a)'] + df_hu['대중교통수단의 편리성(b)'] + df_hu['다른시설과의 연계성(c)'] + df_hu['봉사대상인구수(d)'] + df_hu['여가시설과의 연계성(e)']

# df_hu에서 입지선정계수 구성 항목들 추출 -> df_gis에 있는 후보지들에게 해당 값 추가
# df_gis 는 수요지, 후보지를 모두 갖고 있는 데이터(각 후보지는 입지선정계수 갖고 있음)
cols_to_add = [
    '위도', '경도', '건물명',
    '사회적 안정성(a)',
    '대중교통수단의 편리성(b)',
    '다른시설과의 연계성(c)',
    '봉사대상인구수(d)',
    '여가시설과의 연계성(e)',
    '입지선정계수'
]
df_hu_subset = df_hu[cols_to_add]

df_gis = df_gis.merge(
    df_hu_subset,
    on=['위도', '경도', '건물명'],
    how='left'
)

df_gis

# df_gis에서 수요지, 후보지에 해당하는 건물군들만 필터링 -> 약 3만개 행 필터링(제거) 됨
df_mclp = df_gis[
    (df_gis['건축물용도명'].isin([
        '공동주택', '기숙사', '단독주택', '업무시설', '근린생활시설', '제1종근린생활시설',
        '제2종근린생활시설', '대규모점포', '공원', '숙박시설', '판매시설', '의료시설',
        '오락시설', '산림시설', '공공체육시설', '문화및집회시설', '운동시설'
    ]))
][[
    '시군구', '행정동', '건축물용도명', '시설구분','건물명', 'geometry', '위도', '경도',
    '사회적 안정성(a)', '대중교통수단의 편리성(b)', '다른시설과의 연계성(c)',
    '봉사대상인구수(d)', '여가시설과의 연계성(e)', '입지선정계수', '군집'
]]

df_mclp.shape

df_mclp.head()

"""# df_mclp 좌표계 설정(기존 좌표계 -> EPSG:5179 좌표계)"""

df_mclp.head()

df_mclp.shape

# 위도, 경도 이용해서 EPSG:5179 좌표계 생성

df_mclp['geo_point'] = (
    gpd
    .GeoSeries.from_xy(df_mclp['경도'], df_mclp['위도'], crs="EPSG:4326")
    .to_crs(epsg=5179)
)
cols = df_mclp.columns.tolist()
geom_idx = cols.index('geometry')
cols.insert(geom_idx + 1, cols.pop(cols.index('geo_point')))
df_mclp = df_mclp[cols]

df_mclp.head()

"""# 0번군집 GAAS + MCLP"""

# WKT 문자열을 Point 객체로 변환
def parse_wkt_or_point(value):
    if isinstance(value, str):
        return wkt.loads(value)
    return value

# GeoDataFrame 생성 및 좌표계 설정 (EPSG:5179)
def prepare_geodataframe(df):
    df['geo_point'] = df['geo_point'].apply(parse_wkt_or_point)
    gdf = gpd.GeoDataFrame(df, geometry='geo_point')
    return gdf.set_crs(epsg=5179)

# 두 지점 간 거리 계산
def calculate_distance(p1, p2):
    return p1.distance(p2)

# 건축물 용도에 따라 수요지와 후보지를 분리
def classify_demand_and_facilities(gdf):
    demand_types = [
        '공동주택','기숙사','단독주택','업무시설','근린생활시설',
        '제1종근린생활시설','제2종근린생활시설','대규모점포','공원',
        '숙박시설','판매시설','의료시설'
    ]
    facility_types = [
        '오락시설','산림시설','공공체육시설','문화및집회시설','운동시설'
    ]
    demands    = gdf[gdf['건축물용도명'].isin(demand_types)]
    facilities = gdf[gdf['건축물용도명'].isin(facility_types)]
    return demands, facilities

# Greedy Algorithm을 통해 거리 기준으로 겹치지 않는 후보지 선택
def greedy_adding_algorithm(facilities, num_initial, cover_dist):
    selected, remaining = [], facilities.copy()
    while len(selected) < num_initial and not remaining.empty:
        best = remaining.loc[remaining['입지선정계수'].idxmax()]
        selected.append(best)
        remaining = remaining[
            remaining['geo_point']
            .apply(lambda p: calculate_distance(p, best['geo_point']) > cover_dist)
        ]
    if len(selected) < num_initial:
        print(f"경고: 요청한 {num_initial}개 중 {len(selected)}개만 선정되었습니다.")
    return selected

# MCLP 알고리즘: 주어진 후보지 중에서 k개 조합을 뽑아 최대 커버리지 계산
def mclp(selected, demands, cover_dist, ks):
    coords = np.vstack(demands.geometry.apply(lambda p: (p.x, p.y)).to_numpy())
    tree = KDTree(coords)
    cover_idx = []
    for fac in selected:
        fx, fy = fac['geo_point'].x, fac['geo_point'].y
        idxs = tree.query_radius([[fx, fy]], r=cover_dist)[0]
        cover_idx.append(set(idxs))

    coverage_dict = {}
    n_demands = len(demands)
    for k in ks:
        best_pct, best_combo = 0, None
        for combo_idxs in combinations(range(len(selected)), k):
            union = set().union(*(cover_idx[i] for i in combo_idxs))
            pct = 100 * len(union) / n_demands
            if pct > best_pct:
                best_pct, best_combo = pct, combo_idxs
        chosen = [selected[i] for i in best_combo]
        coverage_dict[k] = {
            'coverage_percentage': best_pct,
            'selected_facilities': chosen
        }
    return coverage_dict

# 커버리지 비율을 선 그래프로 시각화
def plot_coverage_graph(coverage_dict):
    ks  = list(coverage_dict.keys())
    pct = [coverage_dict[k]['coverage_percentage'] for k in ks]
    plt.figure(figsize=(12,8))
    sns.lineplot(x=ks, y=pct, marker='o', label='Coverage')
    plt.scatter(ks, pct, color='red', zorder=5)
    plt.title('시설 개수별 커버리지 비율')
    plt.xlabel('시설 개수')
    plt.ylabel('커버리지 비율 (%)')
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.grid(True)
    plt.show()

# 커버리지 비율을 막대 그래프로 시각화
def plot_bar_graph(coverage_dict):
    ks  = list(coverage_dict.keys())
    pct = [coverage_dict[k]['coverage_percentage'] for k in ks]
    plt.figure(figsize=(12,8))
    palette = sns.color_palette("Blues_r", n_colors=len(ks))
    bars = plt.bar(ks, pct, color=palette, edgecolor='black', linewidth=0.8)
    for b in bars:
        h = b.get_height()
        plt.text(b.get_x()+b.get_width()/2, h+1, f"{h:.2f}%", ha='center')
    plt.title('시설 개수별 커버리지 비율')
    plt.xlabel('시설 개수')
    plt.ylabel('커버리지 비율 (%)')
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.show()

# Folium을 활용한 수요지 및 후보지 시각화 지도 생성
def visualize_folium(demands, selected_facilities, cover_dist):
    demands_ll = demands.to_crs(epsg=4326)
    center = [demands_ll.geometry.y.mean(), demands_ll.geometry.x.mean()]
    m = folium.Map(location=center, zoom_start=12)
    for pt in demands_ll.geometry:
        folium.CircleMarker((pt.y, pt.x), radius=3, color='blue',
                            fill=True, fill_opacity=0.6).add_to(m)
    fac_series = gpd.GeoSeries([f['geo_point'] for f in selected_facilities],
                               crs=5179).to_crs(epsg=4326)
    for pt in fac_series:
        folium.Circle((pt.y, pt.x), radius=cover_dist,
                      color='green', fill=False, weight=2).add_to(m)
        folium.CircleMarker((pt.y, pt.x), radius=6, color='red',
                            fill=True, fill_opacity=0.9).add_to(m)
    return m

# 선택된 후보지를 데이터프레임 형식으로 정리
def create_optimal_facility_dfs(coverage_dict):
    dfs = {}
    for k, info in coverage_dict.items():
        dfs[k] = pd.DataFrame({
            '행정동':    [f['행정동']    for f in info['selected_facilities']],
            '시설구분':  [f['시설구분']  for f in info['selected_facilities']],
            '건물명':    [f['건물명']    for f in info['selected_facilities']],
            'geo_point': [f['geo_point'] for f in info['selected_facilities']],
        })
    return dfs

# 전체 분석 파이프라인 실행 함수
def run_analysis(
    df_gis,
    greedy_coverage_distance,
    num_initial_candidates,
    mclp_coverage_distance,
    num_final_facilities
):
    if isinstance(num_final_facilities, int):
        ks = list(range(1, num_final_facilities+1))
        final_k = num_final_facilities
    else:
        ks = num_final_facilities
        final_k = ks[-1]

    gdf = prepare_geodataframe(df_gis)
    demands, facilities = classify_demand_and_facilities(gdf)
    selected = greedy_adding_algorithm(facilities, num_initial_candidates, greedy_coverage_distance)

    if len(selected) < min(ks):
        ks = [len(selected)]
        final_k = ks[0]
        print(f"후보 부족, k를 {len(selected)}으로 조정")

    coverage_dict = mclp(selected, demands, mclp_coverage_distance, ks)

    plot_coverage_graph(coverage_dict)
    plot_bar_graph(coverage_dict)

    optimal_dfs = create_optimal_facility_dfs(coverage_dict)
    for k in ks:
        print(f"\n시설 개수: {k}")
        print(optimal_dfs[k])

    return visualize_folium(
        demands, coverage_dict[final_k]['selected_facilities'], mclp_coverage_distance
    )

# 0번 군집
mclp_0 = df_mclp[df_mclp['군집']==0]

mclp_0

mclp_0[mclp_0['건축물용도명'].isin(['오락시설','산림시설','공공체육시설','문화및집회시설','운동시설'])]

# GAAS의 초기 후보지의 반경은 500으로 좁게 잡음 -> 후보지 소실을 최소화 하기 위해서
# MCLP 알고리즘의 최종 후보지 반경은 일반 도서관의 유효 반경인 2km의 절반인 1km로 할당 -> 스마트 도서관의 경우 일반 도서관 보다 영향력이 덜 할 것이라 판단했기 때문
# 최종 시설물 개수 5개는 울산시 스마트 도서관 설치 예산 1억을 고려하여 책정

run_analysis(mclp_0,500,25,1000,5)

"""# 3번군집 GAAS + MCLP"""

mclp_3 = df_mclp[df_mclp['군집'] == 3]

mclp_3

run_analysis(mclp_3,500,20,1000,5)

